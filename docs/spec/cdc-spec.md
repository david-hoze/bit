# Content-Defined Chunking for Bare-Layout CAS

## Problem Statement

When a user modifies a small region of a large binary file — appending 30 seconds to a 2 GB video, editing one layer of a 500 MB PSD, fixing a header in a 1 GB dataset — bit currently uploads the entire file again. The CAS stores whole-file blobs keyed by MD5, so any byte change produces a new hash and a new blob. For a user on a 10 Mbps upstream link, re-uploading 2 GB takes ~27 minutes, even though only a few MB actually changed.

Content-defined chunking (CDC) solves this by splitting files into content-determined pieces. When a file changes, only the chunks spanning the edited region get new hashes — the rest are already in the CAS. Push uploads only the missing chunks. Pull downloads only the chunks the local CAS lacks.

**Scope:** CDC applies to bare-layout remotes only. Full-layout remotes maintain human-readable files on dumb storage, and reassembly is impossible without compute on the remote. Metadata-only remotes transfer no content at all.

---

## Design Constraints

1. **Dumb storage.** Remotes have no compute capability. All chunking, reassembly, and dedup logic runs locally. The remote is a key-value store accessed via rclone.
    
2. **Incremental adoption.** Enabling CDC on an existing repo must not break anything. Whole-file blobs and chunk blobs coexist in the same CAS.
    
3. **No new external dependencies.** FastCDC is implemented directly — the algorithm is ~50 lines of core Haskell plus a 256-entry lookup table.
    
4. **Stand on git and rclone.** CDC is one of the rare pieces of novel logic in bit. Keep the implementation minimal: a chunked CAS, not a deduplication system.
    

---

## Algorithm: FastCDC

FastCDC (Gear-hash-based content-defined chunking) is the chunking algorithm. It is simple, fast (single pass, one table lookup + shift + XOR per byte), and well-understood.

### Core Operation

FastCDC slides over file bytes and maintains a rolling fingerprint:

```
fp = (fp << 1) + GEAR_TABLE[byte]
```

When `fp & mask == 0`, a chunk boundary is declared. Two masks are used — a "small mask" with more bits set (harder to match, producing larger chunks) for bytes before the expected average size, and a "large mask" with fewer bits set (easier to match, producing smaller chunks) for bytes after the expected average size. This normalizes chunk sizes toward the average.

### Parameters

```
Parameter   Meaning                          Default
──────────  ───────────────────────────────  ────────
minSize     Minimum chunk size (bytes)       32 KB
avgSize     Expected average chunk size      128 KB
maxSize     Maximum chunk size (bytes)       512 KB
```

**Rationale for defaults:** bit targets large binary files (videos, datasets, images, game assets). The 8 KB average common in dedup literature is designed for VM images and backup systems where fine-grained dedup matters. For bit, larger chunks reduce manifest size (fewer entries per file), reduce per-chunk overhead (each chunk is an rclone operation), and are still small enough to capture localized edits. A 2 GB file at 128 KB average produces ~16,384 chunks — manageable for manifests and rclone batch operations.

The ratio minSize:avgSize:maxSize = 1:4:16 follows FastCDC's recommendation for good size distribution.

### Gear Hash Table

The Gear table is 256 pseudorandom `Word64` values. To ensure deterministic, reproducible chunking across all implementations:

```haskell
-- Bit/CDC/Gear.hs
-- Generated by: for i in 0..255, take first 8 bytes of MD5("gear" ++ show i)
-- interpreted as little-endian Word64.
gearTable :: Array Word8 Word64
gearTable = listArray (0, 255)
  [ 0x..., 0x..., ... ]  -- 256 entries
```

The exact table is fixed as a literal in source code. No runtime generation.

**Regeneration:** `cabal exec -- runghc -package-db=dist-newstyle/packagedb/ghc-9.6.7 GenGear.hs`

**Why MD5?** For a gear hash table, the values just need to be well-distributed
pseudorandom 64-bit numbers. Both MD5 and SHA-256 are cryptographic hashes with
excellent avalanche properties — every output bit depends on every input bit,
and the output is statistically indistinguishable from random. MD5's known
weaknesses are about collision resistance (finding two inputs with the same
hash), which is completely irrelevant here — we use it as a PRNG to generate
256 fixed constants. The bit distribution is equally good either way. MD5 is
already a project dependency (`cryptohash-md5`), making the generation simpler
with no additional dependencies.

### Mask Computation

```haskell
-- Number of bits to match = log2(avgSize)
-- Small mask (pre-normal): set (bits + 1) low bits → harder to match
-- Large mask (post-normal): set (bits - 1) low bits → easier to match

maskS :: Int -> Word64
maskS avgSize = (1 `shiftL` (logBase2 avgSize + 1)) - 1

maskL :: Int -> Word64
maskL avgSize = (1 `shiftL` (logBase2 avgSize - 1)) - 1

logBase2 :: Int -> Int
logBase2 n = finiteBitSize n - 1 - countLeadingZeros n
```

### Algorithm Pseudocode

```
function chunkFile(data, minSize, avgSize, maxSize):
    normalSize = avgSize  -- the crossover point between masks
    smallMask  = maskS(avgSize)
    largeMask  = maskL(avgSize)
    offset     = 0

    while offset < len(data):
        remaining = len(data) - offset
        chunkEnd  = offset + min(remaining, maxSize)
        i         = offset + minSize   -- skip minSize (optimization)
        fp        = 0

        -- Phase 1: bytes [offset+minSize, offset+normalSize) use smallMask
        while i < min(offset + normalSize, chunkEnd):
            fp = (fp << 1) + GEAR_TABLE[data[i]]
            if (fp & smallMask) == 0:
                break at i+1
            i++

        -- Phase 2: bytes [offset+normalSize, chunkEnd) use largeMask
        while i < chunkEnd:
            fp = (fp << 1) + GEAR_TABLE[data[i]]
            if (fp & largeMask) == 0:
                break at i+1
            i++

        emit chunk [offset, i)
        offset = i
```

The minimum-size skip is key: bytes before `offset + minSize` are not examined at all, which both enforces the minimum and improves speed.

---

## Haskell Types

### Chunk Configuration

```haskell
-- Bit/CDC/Types.hs

data ChunkConfig = ChunkConfig
  { ccMinSize :: !Int   -- minimum chunk size in bytes
  , ccAvgSize :: !Int   -- expected average chunk size in bytes
  , ccMaxSize :: !Int   -- maximum chunk size in bytes
  } deriving (Show, Eq)

defaultChunkConfig :: ChunkConfig
defaultChunkConfig = ChunkConfig
  { ccMinSize =  32768   -- 32 KB
  , ccAvgSize = 131072   -- 128 KB
  , ccMaxSize = 524288   -- 512 KB
  }
```

### Chunk Descriptor

```haskell
data Chunk = Chunk
  { chunkOffset :: !Int64      -- byte offset in source file
  , chunkLength :: !Int        -- chunk size in bytes
  , chunkHash   :: Hash 'MD5   -- content hash of this chunk
  } deriving (Show, Eq)
```

**Why MD5 for chunk hashes:** The existing CAS, metadata format, and all verification logic use MD5. Introducing SHA-256 for chunks while keeping MD5 for whole-file identity creates two hash worlds with complex interaction. Chunks are stored in the same CAS namespace as whole-file blobs, and the CAS is self-verifying (filename = hash). MD5's collision weakness is not a practical concern here — bit is a version control system for personal/team use, not a security system. The threat model is accidental corruption, not adversarial collision. If bit ever migrates to SHA-256, it should do so uniformly (whole-file + chunks), not piecemeal.

**Why not SHA-256 for chunks?**

- All existing code paths, the metadata format (`hash: md5:...`), the `Hash` newtype, and rclone's `--checksum` mode use MD5.
- A mixed-hash CAS would require the manifest to declare hash type per entry.
- SHA-256 is ~2x slower than MD5 per byte — marginal in absolute terms but unnecessary given the threat model.
- A future `bit cas rehash` migration can upgrade everything at once.

### Chunk Manifest

```haskell
-- Bit/CDC/Manifest.hs

data ChunkManifest = ChunkManifest
  { cmFileHash   :: Hash 'MD5   -- whole-file hash (matches metadata)
  , cmFileSize   :: Int64       -- original file size in bytes
  , cmChunkCount :: Int         -- number of chunks
  , cmChunks     :: [ChunkRef]  -- ordered list of chunk references
  } deriving (Show, Eq)

data ChunkRef = ChunkRef
  { crHash   :: Hash 'MD5   -- chunk content hash
  , crLength :: !Int         -- chunk size in bytes
  } deriving (Show, Eq)
```

Note: `chunkOffset` is not stored in the manifest. Offsets are implicit — chunks are ordered, and offset is the running sum of preceding lengths. This keeps the manifest compact and avoids offset consistency bugs.

---

## Chunk Storage Format

### CAS Path Scheme

Chunks are stored in the same CAS namespace as whole-file blobs:

```
.bit/cas/<2-char-prefix>/<full-md5-hex>
```

A chunk blob is indistinguishable from a whole-file blob at the CAS level — both are just content-addressed blobs. This is intentional:

- **Self-verifying.** Filename = hash. If it exists, it's correct.
- **Natural deduplication.** If a chunk happens to have the same hash as a whole file (unlikely but valid), they share storage.
- **No CAS schema change.** Existing `hasBlobInCas`, `writeBlobToCas`, `copyBlobFromCasTo` work unchanged for individual chunks.

The manifest — not the CAS — carries the knowledge of which blobs are chunks of which files.

**Why not `cas/chunks/<prefix>/<hash>`?**

- Separating chunks from whole-file blobs requires every CAS operation to know whether it's dealing with a chunk or a whole file.
- Two CAS directories doubles the `rclone lsf` cost for remote inventory.
- A blob is a blob. The CAS should not care about provenance.

### Remote CAS Path (Bare Layout)

Same scheme: `cas/<prefix>/<hash>`. Remote and local CAS are structurally identical. Chunks on the remote are just blobs at their hash-derived path.

---

## Chunk Manifest Format

### On-Disk Format

The manifest is a text file with a fixed header and one line per chunk:

```
file-hash: md5:a1b2c3d4e5f67890a1b2c3d4e5f67890
file-size: 2147483648
chunk-count: 16384
md5:f1e2d3c4b5a69870f1e2d3c4b5a69870 131072
md5:0a1b2c3d4e5f67890a1b2c3d4e5f67890 131072
md5:9f8e7d6c5b4a3210fedcba9876543210 131072
...
md5:1234567890abcdef1234567890abcdef 98304
```

Each chunk line is `<hash> <length>\n`. Order matters — chunks are concatenated in listed order to reconstruct the file.

```haskell
-- Bit/CDC/Manifest.hs

writeManifest :: ChunkManifest -> ByteString
writeManifest ChunkManifest{..} = BS.intercalate "\n" $
  [ "file-hash: " <> encodeUtf8 (hashToText cmFileHash)
  , "file-size: " <> BS8.pack (show cmFileSize)
  , "chunk-count: " <> BS8.pack (show cmChunkCount)
  ] <> map (\ChunkRef{..} ->
       encodeUtf8 (hashToText crHash) <> " " <> BS8.pack (show crLength)
     ) cmChunks

parseManifest :: ByteString -> Either String ChunkManifest
```

### Manifest Storage

Manifests are stored in the CAS, keyed by the whole-file hash with a `.manifest` suffix:

```
Local:  .bit/cas/<prefix>/<whole-file-hash>.manifest
Remote: cas/<prefix>/<whole-file-hash>.manifest
```

For example, a file with hash `md5:a1b2c3d4e5f67890a1b2c3d4e5f67890`:

```
.bit/cas/a1/a1b2c3d4e5f67890a1b2c3d4e5f67890.manifest
```

**Rationale:** Storing the manifest in the CAS alongside the blobs it references keeps the CAS self-contained. The manifest is pushed and pulled as part of the CAS sync. No changes to `.bit/index/` metadata format are needed — the metadata still says `hash: md5:... / size: ...` and the manifest is discovered by convention (append `.manifest` to the blob path).

**Why not in `.bit/index/`?**

- `.bit/index/` is a Git working tree. Adding binary-ish manifests (thousands of lines of hashes) bloats Git history for an implementation detail.
- The metadata format stays clean: `hash:` + `size:`. CDC is a storage optimization, not a metadata change.
- Manifests belong with the data they describe.

**Why not as a special CAS blob (hashed by its own content)?**

- Finding the manifest for a file would require a secondary index.
- Convention-based lookup (file hash + `.manifest`) is simple and O(1).

### Manifest Integrity

The manifest's own integrity is protected by two mechanisms:

1. **Internal consistency.** `file-size` must equal the sum of all chunk lengths. `chunk-count` must equal the number of chunk lines. Parsing rejects inconsistent manifests.
    
2. **Whole-file hash verification.** After reassembling a file from chunks, the result is hashed and compared to `file-hash` in the manifest (which matches the `hash:` in `.bit/index/` metadata). This is the ground truth. A corrupted or tampered manifest will produce a file whose hash doesn't match metadata, and `bit verify` will catch it.
    

---

## Integration with `bit add`

### When Chunking Happens

Chunking applies when ALL of the following are true:

1. Mode is `solid` (bit-lite has no CAS at all).
2. The file is classified as binary (text files store content directly in `.bit/index/`).
3. CDC is enabled in config (`cdc: true`).
4. The file size exceeds `ccMinSize` (32 KB by default). Files smaller than the minimum chunk size would produce exactly one chunk — no benefit.

```haskell
shouldChunk :: BitConfig -> FileEntry -> Bool
shouldChunk cfg fe =
     configMode cfg == Solid
  && not (feIsText fe)
  && configCDC cfg
  && feSize fe > fromIntegral (ccMinSize (configChunkConfig cfg))
```

### Modified `bit add` Flow

Today (mode=solid, no CDC):

```
1. Hash file → md5:...
2. Write metadata to .bit/index/<path>
3. writeBlobToCas casDir filePath hash
```

With CDC enabled:

```
1. Hash file → md5:... (whole-file hash, unchanged)
2. Write metadata to .bit/index/<path> (unchanged: hash + size)
3. If shouldChunk:
   a. chunkFile config filePath → [Chunk]
   b. For each chunk: write chunk blob to CAS
   c. Build ChunkManifest from chunks + whole-file hash + size
   d. Write manifest to CAS at <hash>.manifest
4. Else:
   writeBlobToCas casDir filePath hash (unchanged)
```

The whole-file hash in metadata is always computed, regardless of CDC. It is the file's identity. CDC is purely a storage optimization beneath that identity.

```haskell
addFileToCas :: BitConfig -> FilePath -> FilePath -> Hash 'MD5 -> Int64 -> IO ()
addFileToCas cfg casDir filePath fileHash fileSize
  | shouldChunkBySize cfg fileSize = do
      chunks <- chunkFile (configChunkConfig cfg) filePath
      forM_ chunks $ \Chunk{..} -> do
        let blobPath = casBlobPath casDir chunkHash
        unlessM (doesFileExist blobPath) $
          writeChunkBlob casDir filePath chunkOffset chunkLength chunkHash
      let manifest = buildManifest fileHash fileSize chunks
      writeManifestToCas casDir fileHash manifest
  | otherwise =
      writeBlobToCas casDir filePath fileHash

shouldChunkBySize :: BitConfig -> Int64 -> Bool
shouldChunkBySize cfg size =
  configCDC cfg && size > fromIntegral (ccMinSize (configChunkConfig cfg))
```

### Streaming Chunking

The chunking operation reads the file in a single streaming pass. It does NOT load the entire file into memory:

```haskell
-- Bit/CDC/FastCDC.hs

chunkFile :: ChunkConfig -> FilePath -> IO [Chunk]
chunkFile cfg path = withFile path ReadMode $ \h -> do
    fileSize <- hFileSize h
    go h 0 fileSize []
  where
    go h offset remaining acc
      | remaining <= 0 = pure (reverse acc)
      | otherwise = do
          let toRead = fromIntegral (min remaining (fromIntegral (ccMaxSize cfg)))
          buf <- BS.hGet h toRead
          let (chunkLen, _) = findBoundary cfg buf (fromIntegral remaining)
              chunkData = BS.take chunkLen buf
              hash = hashBytes chunkData
              chunk = Chunk offset chunkLen hash
          -- Seek back if we read past the boundary
          when (chunkLen < BS.length buf) $
            hSeek h RelativeSeek (fromIntegral (chunkLen - BS.length buf))
          go h (offset + fromIntegral chunkLen)
                (remaining - fromIntegral chunkLen)
                (chunk : acc)
```

The pure boundary-finding function:

```haskell
-- Bit/CDC/FastCDC.hs

findBoundary :: ChunkConfig -> ByteString -> Int -> (Int, Word64)
findBoundary ChunkConfig{..} buf remaining = go ccMinSize 0
  where
    len       = min (BS.length buf) (min remaining ccMaxSize)
    normalPos = min ccAvgSize len
    smask     = maskS ccAvgSize
    lmask     = maskL ccAvgSize

    go !i !fp
      | i >= len  = (len, fp)                   -- hit maxSize or EOF
      | i < normalPos =                          -- phase 1: small mask
          let fp' = (fp `shiftL` 1) + gearTable V.! fromIntegral (BS.index buf i)
          in if fp' .&. smask == 0
             then (i + 1, fp')
             else go (i + 1) fp'
      | otherwise =                              -- phase 2: large mask
          let fp' = (fp `shiftL` 1) + gearTable V.! fromIntegral (BS.index buf i)
          in if fp' .&. lmask == 0
             then (i + 1, fp')
             else go (i + 1) fp'
```

---

## Integration with `bit push` (Bare Layout)

### Current Flow (No CDC)

```
For each file where local hash ≠ remote hash:
  rclone copyto .bit/cas/<prefix>/<hash> remote:cas/<prefix>/<hash>
Push metadata bundle.
```

### CDC-Aware Push Flow

```
For each new/modified file:
  1. Read manifest from local CAS: .bit/cas/<prefix>/<hash>.manifest
     If no manifest exists → file is whole-blob, push as today.
  2. Collect all chunk hashes from manifest.
  3. Determine which chunks remote already has (see "Remote Chunk Index").
  4. Upload missing chunks via rclone (batched).
  5. Upload manifest.
Push metadata bundle.
```

```haskell
pushChunkedFile :: Remote -> FilePath -> Hash 'MD5 -> IO ()
pushChunkedFile remote casDir fileHash = do
    let manifestPath = casManifestPath casDir fileHash
    hasManifest <- doesFileExist manifestPath
    if hasManifest
      then do
        manifest <- readManifestFromCas casDir fileHash
        let allChunkHashes = map crHash (cmChunks manifest)
        remoteHas <- queryRemoteChunks remote allChunkHashes
        let missing = filter (`S.notMember` remoteHas) allChunkHashes
        -- Batch upload missing chunks
        uploadBlobsBatched remote casDir missing
        -- Upload manifest
        uploadManifest remote casDir fileHash
      else
        -- Whole-file blob, existing behavior
        uploadBlob remote casDir fileHash
```

### Remote Chunk Index

The expensive question: "Which chunks does the remote already have?"

**Option A: `rclone lsf` the entire CAS.** For a remote with 100,000 blobs, this returns 100,000 lines. Parsing is fast, but the listing itself takes seconds to minutes depending on the remote backend (S3: fast; Google Drive: slow). This is the simple approach and what bit uses for the first implementation.

**Option B: Remote manifest index.** Maintain a file `cas/chunk-index.txt` on the remote listing all blob hashes. Updated on each push. Downloading one file is faster than listing an entire directory tree. But it introduces a mutable shared file on dumb storage — concurrent pushes can corrupt it.

**Option C: Local tracking.** Remember which blobs have been pushed to each remote in a local file (`.bit/remotes/<name>/pushed-blobs`). No remote query needed. But becomes stale if another client pushes to the same remote.

**Decision: Start with Option A, optimize later.** The first implementation uses `rclone lsf --recursive cas/` to get the full set of remote blob paths, parses out hashes, and builds an in-memory `Set (Hash 'MD5)`. This is correct in all cases (including concurrent pushes from multiple clients) and requires no new remote-side state.

```haskell
-- Bit/Remote/ChunkIndex.hs

queryRemoteBlobs :: Remote -> IO (Set (Hash 'MD5))
queryRemoteBlobs remote = do
    -- rclone lsf --recursive <remote>:cas/
    output <- rcloneLsf (remoteTarget remote </> "cas") ["--recursive"]
    let hashes = mapMaybe parseBlobHash (T.lines output)
    pure (S.fromList hashes)

parseBlobHash :: Text -> Maybe (Hash 'MD5)
parseBlobHash line =
    -- Lines look like "a1/a1b2c3d4e5f67890a1b2c3d4e5f67890"
    -- Skip .manifest files
    let filename = T.takeWhileEnd (/= '/') line
    in if ".manifest" `T.isSuffixOf` filename
       then Nothing
       else Just (Hash ("md5:" <> filename))
```

**Future optimization:** Option C (local push tracking) is added as a transparent cache in front of the `rclone lsf`. The local pushed-blobs set is consulted first; `rclone lsf` is the fallback for cache misses or stale state. This optimization is documented here for future implementation but is not part of the initial CDC spec.

### Batched Chunk Upload

Missing chunks are uploaded in a single `rclone copy --files-from` call, matching the existing batch upload pattern for non-CAS files:

```haskell
uploadBlobsBatched :: Remote -> FilePath -> [Hash 'MD5] -> IO ()
uploadBlobsBatched remote casDir hashes = do
    -- Write a files-from list: each line is the relative CAS path
    let lines = map (\h -> let p = casBlobRelPath h in p) hashes
    writeFilesFrom tmpFile lines
    rcloneCopy casDir (remoteTarget remote) ["--files-from", tmpFile, "--transfers", "32"]
```

This collapses potentially thousands of chunk uploads into one rclone subprocess — critical for performance on high-latency remotes. The `--transfers 32` flag runs 32 concurrent uploads within that single subprocess. CAS chunks are small (~128KB each), so each upload is latency-bound, not bandwidth-bound. With 32 parallel transfers at ~30ms round-trip, 1,775 PUTs take ~1.7s in pure latency overhead instead of ~13s at the rclone default of 4 transfers. This flag is applied only to CAS chunk operations — whole-file copies use the default parallelism since they are bandwidth-bound.

---

## Integration with `bit pull` (Bare Layout)

### Current Flow (No CDC)

```
Fetch metadata bundle, merge.
For each file where local hash ≠ working tree:
  rclone copyto remote:cas/<prefix>/<hash> .bit/cas/<prefix>/<hash>
  Copy from CAS to working tree path.
```

### CDC-Aware Pull Flow

```
Fetch metadata bundle, merge.
For each file needed:
  1. Check if remote has a manifest: remote:cas/<prefix>/<hash>.manifest
     If no manifest → file is whole-blob, pull as today.
  2. Download manifest.
  3. Parse manifest. Collect chunk hashes.
  4. Check which chunks are already in local CAS.
  5. Download missing chunks (batched).
  6. Reassemble file from chunks into working tree.
  7. Verify reassembled file hash matches metadata.
```

```haskell
pullChunkedFile :: Remote -> FilePath -> Hash 'MD5 -> FilePath -> IO Bool
pullChunkedFile remote casDir fileHash destPath = do
    -- Try to download manifest
    let remoteManifestPath = remoteCasManifestPath remote fileHash
    manifestExists <- rcloneFileExists remoteManifestPath
    if manifestExists
      then do
        downloadManifest remote casDir fileHash
        manifest <- readManifestFromCas casDir fileHash
        let needed = filter (not <$> hasBlobInCas casDir . crHash) (cmChunks manifest)
        let missingHashes = map crHash needed
        downloadBlobsBatched remote casDir missingHashes
        reassembleFile casDir manifest destPath
      else do
        -- Whole-file blob, existing behavior
        downloadBlob remote casDir fileHash
        copyBlobFromCasTo casDir fileHash destPath
```

### File Reassembly

```haskell
-- Bit/CDC/Reassemble.hs

reassembleFile :: FilePath -> ChunkManifest -> FilePath -> IO ()
reassembleFile casDir ChunkManifest{..} destPath =
    withFile destPath WriteMode $ \hOut ->
      forM_ cmChunks $ \ChunkRef{..} -> do
        let blobPath = casBlobPath casDir crHash
        chunkData <- BS.readFile blobPath
        when (BS.length chunkData /= crLength) $
          throwIO $ ChunkSizeMismatch crHash crLength (BS.length chunkData)
        BS.hPut hOut chunkData

-- After reassembly, verify:
verifyReassembledFile :: FilePath -> Hash 'MD5 -> IO ()
verifyReassembledFile path expectedHash = do
    actualHash <- hashFile path
    when (actualHash /= expectedHash) $
      throwIO $ ReassemblyHashMismatch expectedHash actualHash
```

Reassembly is streaming: chunks are read and written sequentially. Peak memory usage is one chunk (≤512 KB by default), not the entire file.

---

## Backward Compatibility

### Mixed CAS Content

The CAS may contain:

- Whole-file blobs (pre-CDC or files below the size threshold)
- Chunk blobs (CDC-produced)
- Manifest files (`<hash>.manifest`)

These coexist without conflict. The CAS is just a flat content-addressed store; it doesn't know or care about blob provenance.

### How bit Distinguishes Whole-File from Chunked

The presence of a `.manifest` file is the signal:

```
Has .bit/cas/<prefix>/<hash>.manifest?
  Yes → file is chunked. Read manifest, operate on chunks.
  No  → file is a whole-file blob. Operate as today.
```

This check is performed at push time (local CAS) and pull time (remote CAS). The metadata in `.bit/index/` is identical either way — `hash:` + `size:`.

```haskell
isChunkedFile :: FilePath -> Hash 'MD5 -> IO Bool
isChunkedFile casDir fileHash =
    doesFileExist (casManifestPath casDir fileHash)
```

### No Mandatory Migration

Existing repos continue to work without any change. CDC is opt-in via configuration. When CDC is disabled (the default), `bit add` stores whole-file blobs exactly as today. When CDC is enabled, only newly-added or modified files are chunked. Existing whole-file blobs remain in the CAS and are served normally on pull.

### Optional Re-Chunking

A `bit cas rechunk` command (future, not in initial implementation) re-chunks existing whole-file blobs:

```
$ bit cas rechunk
Scanning CAS for whole-file blobs...
Found 47 files eligible for chunking (total: 12.3 GB)
Chunking: lectures/lecture-07.mp3 (523 MB) → 4091 chunks
Chunking: dataset/train.csv (2.1 GB) → 16384 chunks
...
Done. Freed 0 bytes locally (originals retained until verified push).
```

This is a local operation. It chunks each whole-file blob, writes the manifest, but does **not** delete the original whole-file blob (it may still be referenced by a remote that doesn't have the chunks). Garbage collection of orphaned whole-file blobs is a separate concern.

---

## Verification and Integrity

### `bit verify`

`bit verify` already checks that every file in metadata has a corresponding CAS blob. With CDC, the check extends:

```
For each file in metadata:
  hash = metadata hash
  If manifest exists at <hash>.manifest:
    Parse manifest.
    Verify manifest.file-hash == metadata hash.
    Verify manifest.file-size == metadata size.
    Verify sum(chunk lengths) == file-size.
    Verify chunk-count == number of chunk lines.
    For each chunk in manifest:
      Verify chunk blob exists in CAS.
      Verify chunk blob size == declared length.
    (Optional, --deep): Reassemble and verify whole-file hash.
  Else:
    Verify whole-file blob exists in CAS (existing behavior).
```

```haskell
data VerifyDepth = Quick | Deep

verifyFile :: VerifyDepth -> FilePath -> Hash 'MD5 -> Int64 -> IO [VerifyError]
verifyFile depth casDir fileHash fileSize = do
    chunked <- isChunkedFile casDir fileHash
    if chunked
      then verifyChunkedFile depth casDir fileHash fileSize
      else verifyWholeFile casDir fileHash
```

The `--deep` flag triggers full reassembly and hash verification. Without it, verify only checks structural consistency (manifest parses, blobs exist, sizes match). Deep verification is expensive for large files but provides the strongest guarantee.

### `bit fsck`

`bit fsck` performs deep verification on all files and attempts repair:

- Missing chunks: report which chunks are missing and which files are affected. Suggest `bit pull <remote>` to fetch missing chunks.
- Corrupted chunks (size mismatch): delete and re-fetch.
- Corrupted manifest (parse failure or inconsistency): delete manifest. On next `bit add` or `bit pull`, the manifest will be regenerated.

### Self-Verification Properties

The integrity chain is:

```
.bit/index/<path>  →  hash: md5:... (ground truth, in Git history)
       ↓
.bit/cas/<prefix>/<hash>.manifest  →  file-hash matches, chunks listed
       ↓
.bit/cas/<prefix>/<chunk-hash>  →  filename = content hash (self-verifying)
       ↓
Reassembled file  →  hash matches metadata (end-to-end verification)
```

Every link in this chain is independently verifiable. Corruption at any level is detectable.

---

## Configuration

### Config Keys

CDC configuration uses git-style INI format in `.bit/config`:

```ini
[core]
    mode = solid          # required for CAS (and thus CDC)

[cdc]
    enabled = true        # content-defined chunking (default: enabled)
    min-size = 32768      # minimum chunk size in bytes (default: 32 KB)
    avg-size = 131072     # average chunk size in bytes (default: 128 KB)
    max-size = 524288     # maximum chunk size in bytes (default: 512 KB)
```

Set via `bit config`:

```
bit config cdc.enabled true
bit config cdc.min-size 32768
bit config cdc.avg-size 131072
bit config cdc.max-size 524288
```

CDC is a repo-level setting, not per-remote. Chunking happens at `bit add` time -- the CAS stores either whole-file blobs or chunks, and all remotes that sync CAS content see the same blobs.

**Why not per-remote?** If remote A expects chunks and remote B expects whole files, the CAS would need to store both forms. This doubles storage and complicates push logic. Instead, CDC is a property of how the local CAS organizes data. Remotes that consume CAS content (bare layout) get whatever the CAS has. Full-layout remotes don't use the CAS for push (they copy readable files), so they're unaffected.

### Validation

```haskell
validateChunkConfig :: ChunkConfig -> Either String ChunkConfig
validateChunkConfig cc@ChunkConfig{..}
  | ccMinSize <= 0       = Left "cdc.min-size must be positive"
  | ccAvgSize <= ccMinSize = Left "cdc.avg-size must be greater than cdc.min-size"
  | ccMaxSize <= ccAvgSize = Left "cdc.max-size must be greater than cdc.avg-size"
  | otherwise            = Right cc
```

### Parameter Summary

```
Key              Type    Default   Constraint
──────────────   ──────  ────────  ──────────────────────────
cdc.enabled      Bool    true      —
cdc.min-size     Int     32768     > 0
cdc.avg-size     Int     131072    > cdc.min-size
cdc.max-size     Int     524288    > cdc.avg-size
```

**Changing chunk parameters after files are already chunked:** Existing manifests and chunks remain valid -- they were produced with the old parameters. New files will be chunked with the new parameters. This means a single CAS may contain chunks of varying sizes. This is fine; the manifest records each chunk's length explicitly.

---

## Cross-File and Cross-Version Deduplication

### How CDC Enables Dedup

CDC chunk boundaries are determined by file content, not position. When two files (or two versions of the same file) share a region of bytes, that region produces the same chunks with the same hashes. The CAS naturally deduplicates: `writeBlobToCas` is a no-op if the blob already exists.

### Expected Benefits by Use Case

```
Use case                          Expected dedup ratio   Why
──────────────────────────────    ────────────────────   ──────────────────────
Video: append/trim                High (80-95%)          Unmodified frames keep
                                                         same chunks. Only
                                                         chunks near edit point
                                                         change.

Dataset: append rows to CSV       High (80-95%)          Existing rows produce
                                                         same chunks. New rows
                                                         add new chunks at end.

PSD/TIFF: edit one layer          Medium (40-70%)        Depends on file format
                                                         internal layout. Some
                                                         formats rewrite headers
                                                         or compress.

Game assets: texture edit         Medium (50-80%)        Uncompressed formats
                                                         (BMP, RAW) dedup well.
                                                         Compressed (PNG, DDS)
                                                         dedup poorly.

Across similar files              Low-Medium (20-50%)    Two similar images may
                                                         share some chunks.
                                                         Depends heavily on
                                                         format and content.
```

### Where CDC Does NOT Help

- **Encrypted files.** Changing one byte changes the entire ciphertext. Zero dedup.
- **Compressed archives (.zip, .tar.gz).** Compression destroys byte-level locality. A one-byte change propagates through the compressed stream. Zero dedup.
- **Re-encoded media (.mp4, .aac).** Lossy re-encoding produces entirely different bytes even for "identical" content. Zero dedup.
- **Format-internal compression.** Formats like .docx (zip of XML), .xlsx, .pptx are zip archives internally. Same problem as .zip.

For these cases, CDC still works correctly (files are chunked and stored), but provides no space or bandwidth savings — every version produces entirely new chunks.

---

## Interaction with Metadata-Only Remotes

Metadata-only remotes (`layout: metadata`) sync only Git history. They transfer no CAS content — no blobs, no chunks, no manifests. CDC does not affect them in any way.

The metadata in `.bit/index/` does not reference chunks or manifests. It contains only `hash:` and `size:`, which are the whole-file identity. A metadata-only clone can display file listings, sizes, and hashes without any CAS content being available.

```
Layout      CAS content synced     CDC applies?
──────────  ─────────────────────  ────────────
full        Whole-file blobs       No (full always uses whole files)
bare        Chunks + manifests     Yes
metadata    Nothing                No (no content at all)
```

---

## Interaction with Full-Layout Remotes

Full-layout remotes copy human-readable files to the remote alongside the CAS. The human-readable copy is always a whole file (e.g., `lectures/lecture-07.mp3`). The CAS on a full-layout remote also stores whole-file blobs, not chunks.

**Why not chunks on full-layout CAS?** Full-layout remotes are designed to be browsable — a user can navigate to `lectures/` on Google Drive and play the file. The CAS on a full-layout remote serves as a recovery mechanism (if the readable copy is deleted). Storing chunks in the full-layout CAS would require reassembly to recover, which defeats the "browse it on the remote" purpose. Full-layout CAS remains whole-file.

If a file was chunked locally and needs to be pushed to a full-layout remote, `bit push` reassembles the file from local chunks into a temporary file, then uploads the whole file to both the readable path and the CAS.

```haskell
pushToFullLayout :: FilePath -> Hash 'MD5 -> FilePath -> Remote -> IO ()
pushToFullLayout casDir fileHash readablePath remote = do
    chunked <- isChunkedFile casDir fileHash
    srcPath <- if chunked
      then do
        manifest <- readManifestFromCas casDir fileHash
        tmpPath <- newTempFile
        reassembleFile casDir manifest tmpPath
        pure tmpPath
      else
        pure (casBlobPath casDir fileHash)
    -- Upload to readable path and CAS (whole-file)
    rcloneCopyTo srcPath (remoteTarget remote </> readablePath)
    rcloneCopyTo srcPath (remoteCasBlobPath remote fileHash)
```

---

## The FastCDC Implementation

### Module: `Bit/CDC/FastCDC.hs`

This module is pure except for file I/O at the top-level `chunkFile` entry point. The core boundary-finding logic is a pure function over `ByteString`.

### Gear Table: `Bit/CDC/Gear.hs`

```haskell
module Bit.CDC.Gear (gearTable) where

import Data.Array (Array, listArray)
import Data.Word (Word8, Word64)

-- | 256-entry Gear hash table. Each entry is the first 8 bytes of
-- MD5("gear" ++ show i) interpreted as little-endian Word64, for i in [0..255].
-- This is a compile-time constant.
gearTable :: Array Word8 Word64
gearTable = listArray (0, 255) [ {- 256 literal Word64 values -} ]
```

The table is generated once by `GenGear.hs` and committed as a literal. Deterministic chunking requires all implementations to use the exact same table. Regenerate with: `cabal exec -- runghc -package-db=dist-newstyle/packagedb/ghc-9.6.7 GenGear.hs`

### Core Types: `Bit/CDC/Types.hs`

```haskell
module Bit.CDC.Types
  ( ChunkConfig(..)
  , defaultChunkConfig
  , Chunk(..)
  , ChunkRef(..)
  , ChunkManifest(..)
  ) where
```

### Chunking Algorithm: `Bit/CDC/FastCDC.hs`

```haskell
module Bit.CDC.FastCDC
  ( chunkFile
  , chunkByteString  -- pure, for testing
  , findBoundary     -- pure, for testing
  ) where

import Bit.CDC.Gear (gearTable)
import Bit.CDC.Types

-- | Chunk a file on disk. Streaming, single-pass. Peak memory: one maxSize
-- buffer.
chunkFile :: ChunkConfig -> FilePath -> IO [Chunk]

-- | Chunk an in-memory ByteString. Pure. Used for testing and small files.
chunkByteString :: ChunkConfig -> ByteString -> [Chunk]

-- | Find the next chunk boundary in a buffer. Pure.
-- Returns (chunkLength, finalFingerprint).
findBoundary :: ChunkConfig -> ByteString -> Int -> (Int, Word64)
```

### Manifest Handling: `Bit/CDC/Manifest.hs`

```haskell
module Bit.CDC.Manifest
  ( writeManifest
  , parseManifest
  , readManifestFromCas
  , writeManifestToCas
  , buildManifest
  , casManifestPath
  ) where

-- | Build a manifest from chunking results.
buildManifest :: Hash 'MD5 -> Int64 -> [Chunk] -> ChunkManifest
buildManifest fileHash fileSize chunks = ChunkManifest
  { cmFileHash   = fileHash
  , cmFileSize   = fileSize
  , cmChunkCount = length chunks
  , cmChunks     = map (\Chunk{..} -> ChunkRef chunkHash chunkLength) chunks
  }

-- | CAS path for a manifest file.
casManifestPath :: FilePath -> Hash 'MD5 -> FilePath
casManifestPath casDir h =
    casBlobPath casDir h <> ".manifest"
```

### Reassembly: `Bit/CDC/Reassemble.hs`

```haskell
module Bit.CDC.Reassemble
  ( reassembleFile
  , verifyReassembledFile
  ) where
```

### CAS Integration: extends `Bit/CAS.hs`

```haskell
-- New exports from Bit/CAS.hs:

writeChunkBlob :: FilePath -> FilePath -> Int64 -> Int -> Hash 'MD5 -> IO ()
-- Writes bytes [offset, offset+length) from source file to CAS at chunk hash.

isChunkedFile :: FilePath -> Hash 'MD5 -> IO Bool
-- Checks for existence of <hash>.manifest.
```

---

## Module Organization Summary

```
Module                      Purpose                         Pure?
──────────────────────────  ──────────────────────────────  ─────
Bit.CDC.Gear                Gear hash lookup table          Yes
Bit.CDC.Types               ChunkConfig, Chunk, manifest    Yes
Bit.CDC.FastCDC             Chunking algorithm              Mostly (chunkFile is IO)
Bit.CDC.Manifest            Manifest read/write/build       Mostly (CAS I/O wrappers)
Bit.CDC.Reassemble          File reassembly + verification  IO
Bit.CAS                     Extended with chunk helpers      IO
Bit.Remote.ChunkIndex       Remote blob inventory           IO
```

The `Bit.CDC.*` modules form a self-contained subsystem. `Bit.CAS` gains a few new exports. The push/pull pipelines in `Bit/Remote/Push.hs` and `Bit/Remote/Pull.hs` are extended with CDC-aware branches.

---

## What We Deliberately Do NOT Do

- **No chunk-level garbage collection.** Orphaned chunks (referenced by no manifest) are not automatically deleted. A future `bit cas gc` can handle this. For now, unused chunks cost only disk space.
    
- **No delta compression between chunks.** CDC provides dedup (identical chunks stored once), not delta encoding (similar chunks stored as diffs). Delta encoding adds substantial complexity for marginal benefit on top of CDC.
    
- **No remote-side reassembly.** Dumb storage cannot run code. Full-layout remotes get whole files. Bare-layout remotes store chunks. There is no "assemble on the remote" step.
    
- **No chunk encryption or signing.** The CAS is not a security boundary. If signing is needed in the future, it should cover the whole CAS, not individual chunks.
    
- **No per-file chunk parameters.** All files use the same `ChunkConfig` from `.bit/config`. Per-file tuning adds complexity with minimal benefit — the default parameters work well for the 1 MB–10 GB file range that bit targets.
    
- **No streaming push/pull of individual chunks.** Chunks are uploaded and downloaded as complete blobs via rclone, not streamed byte-by-byte. rclone handles resumption and retries at the file (blob) level.
    
- **No CDC for bit-lite.** bit-lite has no CAS. CDC requires the CAS. Enabling CDC implicitly requires `mode: solid`.

---

## Benchmark Results

### Test Setup

- **File:** Blender 4.3 splash screen, 227 MB (238,766,205 bytes)
- **CDC config:** default (min 128KB, avg 512KB, max 2MB)
- **Chunks produced:** ~1,370 chunk blobs + 1 manifest per version
- **Remote:** MinIO (localhost S3, <1ms RTT)
- **Platform:** Windows MINGW64, bit with `--transfers 32` for CAS operations

### End-to-End Push/Pull Timing (MinIO)

| Step | Description | Time | Chunks transferred |
|------|-------------|------|--------------------|
| First push | All chunks, empty remote | 4.4s | 1,926 uploaded |
| Incremental push | 6-byte edit at 1MB offset | 3.4s | 2 uploaded, 1,370 skipped (dedup) |
| Incremental push | 4KB edit at 50MB offset | 3.5s | 2 uploaded, 1,371 skipped (dedup) |
| Fresh pull | Clone into new repo | 5.9s | 1,925 downloaded |
| Incremental pull | After another 6-byte edit | 6.3s | 1 downloaded, 1,924 skipped (local cache) |
| Rename push | Rename only, no content change | 3.9s | 1 uploaded (manifest), 1,371 skipped |

The ~3.4s floor on incremental push is overhead from remote state inspection, dedup query (`rclone lsf`), and metadata bundle upload — the actual chunk transfer is near-instant.

### Parallel Transfers A/B Test (MinIO, 1,371 CAS files)

| Transfers | Time | Notes |
|-----------|------|-------|
| 4 (old default) | 2.5s | Latency-bound on cloud; bandwidth-bound on localhost |
| 32 (new) | 1.9s | ~1.3x on localhost (low RTT masks the difference) |

On localhost MinIO (<1ms RTT), the improvement is modest because transfers are bandwidth-bound, not latency-bound. On cloud remotes with 30–100ms RTT, the theoretical speedup is ~8x: at 30ms RTT, 1,371 PUTs take ~10.3s with 4 transfers vs ~1.3s with 32 transfers.

**Google Drive test (inconclusive):** An A/B test against `gdrive-test:` was attempted but did not complete after 28 minutes during the `--transfers 4` phase. This may be due to Google Drive API rate limiting (default: 2 requests/second for file creation) rather than pure latency. Google Drive rate limits would cap both `--transfers 4` and `--transfers 32` at the same throughput, making this remote a poor benchmark for parallelism. The MinIO results isolate the rclone parallelism variable cleanly.

### Scripts

- `test/single-workflows/test-minio-transfers.sh [remote]` — A/B comparison: same chunks, `--transfers 4` vs `--transfers 32`
- `test/single-workflows/test-minio-e2e-timing.sh [remote]` — Full lifecycle: first push, incremental push, clone, incremental pull, verify
- `test/single-workflows/test-minio-push-pull.sh` — Basic push/pull round-trip with CDC